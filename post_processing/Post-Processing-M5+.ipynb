{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import interpolate\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Organize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_filename = 'usgs_full_catalog_times_cmt.txt'\n",
    "extra_info_filename = \"usgs_full_catalog_M5_info.txt\"\n",
    "main_event_times_filename = 'usgs_full_catalog_M5_times.txt'\n",
    "main_events_filename = 'usgs_full_catalog_M5.csv'\n",
    "times_original = np.loadtxt(times_filename, dtype = str)\n",
    "times_dt64 = []\n",
    "\n",
    "for i in times_original:\n",
    "    times_dt64.append(np.datetime64(i[0]+'T'+i[1])) # Convert times to numpy datetime64\n",
    "grand_times = np.array(times_dt64)\n",
    "extra_info = np.loadtxt(extra_info_filename,dtype = object)\n",
    "\n",
    "main_events = np.loadtxt(main_events_filename,skiprows = 1, delimiter = ',')\n",
    "main_times_original = np.loadtxt(main_event_times_filename, dtype = str)\n",
    "times_dt64_2 = []\n",
    "\n",
    "for i in main_times_original:\n",
    "    times_dt64_2.append(np.datetime64(i)) # Convert times to numpy datetime64\n",
    "main_event_times = np.array(times_dt64_2)\n",
    "\n",
    "offshore_indicies = [1,18,88,101,102,103,129,138,21,22,63,69,100,112] # Indicies for main_events matrix that contains events occuring offshore\n",
    "\n",
    "r_list = [0.25,0.5,0.75,1,2,3,4,7,15,30,45,60,90,120,150,180,365,730,1095]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MatthewsC(P,S):\n",
    "    \"\"\"calculate the matthews correlation coefficient.\n",
    "    Expects a vector of predictions P and observations S.\n",
    "    The values in P and S.\n",
    "    P=0 means no larger earthquake earthquake\n",
    "    P=1 means that we predict a larger earthquake\n",
    "    P=.05 means 1/20 chance of a larger earthquake\"\"\"\n",
    "    assert(np.shape(S) == np.shape(P))\n",
    "    sbar = np.mean(S)\n",
    "    pbar = np.mean(P)\n",
    "    if pbar == 0 or sbar == 0 or np.std(S) == 0 or np.std(P) == 0:\n",
    "        return 0\n",
    "    numerator = np.sum((S-sbar)*(P-pbar))\n",
    "    denominator = (np.sum((S-sbar)**2)*np.sum((P-pbar)**2))**0.5\n",
    "    C = numerator/denominator\n",
    "    return C\n",
    "def calculate_accuracy(grand_times,csv_data,r_current,extra_info,main_event_times,main_events,r_list,pair_matrix,index,offshore_indicies,default_prob=.90):\n",
    "    ''' Function takes the results with equivalent end times for a certain parameter combination, determines the \n",
    "    sucsess of the TLS score, counts the confusion matrix, and converts to the different success measures. This is repeated for each\n",
    "    alert threshold within this function, so the output is 5 values for each statistic (f1_list,f2_list,etc), their corresponding confusion\n",
    "    matricies (conf_list), and the results for each earthquake pair (pair matrix)\n",
    "    Inputs are the grand_times = complete time data, the csv file for a particular parameter combination = csv_data,\n",
    "    end time of interest = r_current, the matrix containing event relationships based off the GK window (extra_info),\n",
    "    times of M5+ events (main_event_times), M5+ catalog (main_events), list of all end times (r_list), the indicies of offshore events, so \n",
    "    they may be skipped (offshore_indicies),and the index corresponding to the csv file so that the results are organized \n",
    "    correctly. default_prob=.9 this is the default probability that an event is a mainshock.'''\n",
    "    r_ind = r_list.index(r_current)\n",
    "    percent_change_column = csv_data.iloc[:,8+(9*r_ind)]\n",
    "    cat_end_time = np.max(grand_times) # Latest Time in the Catalog\n",
    "    magnitude_interp = np.arange(2.5,8.5,0.5)\n",
    "    time_function = interpolate.interp1d(magnitude_interp,np.array([6,11.5,22,42,83,155,290,510,790,915,960,985])) # GK Temporal Window\n",
    "    conf_list = [] # Confusion Matrix\n",
    "    f1_list = []\n",
    "    f2_list = []\n",
    "    C_list = []\n",
    "    ktb_list = []\n",
    "    ba_list = []\n",
    "    new_index = np.copy(index)\n",
    "    for threshold in [5,10,15,20,25]: # Loop through Alert Thresholds\n",
    "        # Initialize confusion matrix quantities\n",
    "        true_predict = 0 # True Positives, Correct Predictions\n",
    "        true_nonpredict = 0 # True Negatives, Correct Non-Predictions\n",
    "        false_predict = 0 # False Negatives, False Predictions\n",
    "        failed_to_predict = 0 # False Positives, 2nd Event occured with no prediction\n",
    "        neutral = 0\n",
    "        early = 0\n",
    "        insuff = 0\n",
    "        post_gk_deadline = 0 # Events for which this end-time is past the end of the GK Window\n",
    "        post_second_event_deadline = 0 # Events for which this end-time is past the mainshock is the event is the first earthquake in a foreshock-mainshock pair\n",
    "        prediction_list = []\n",
    "        observation_list = []\n",
    "        if r_current == 0.25:\n",
    "            r_datetime = np.timedelta64(6,'h')\n",
    "        elif r_current == 0.5:\n",
    "            r_datetime = np.timedelta64(12,'h')\n",
    "        elif r_current == 0.75:\n",
    "            r_datetime = np.timedelta64(18,'h')\n",
    "        else:\n",
    "            r_datetime = np.timedelta64(r_current,'D')\n",
    "        pair_event = 0\n",
    "        for i in range(len(percent_change_column)): # Loop through the results for each event\n",
    "            if i in offshore_indicies: # Remove all offshore events from the analysis\n",
    "                if percent_change_column[i] == 'None': # See if an offshore earthquake generates a score, which would be an interesting finding\n",
    "                    if extra_info[i,1] == 'yes':  # If the event is a foreshock-mainshock pair, add it to the pair matrix\n",
    "                        pair_matrix[new_index,pair_event] = 'off'\n",
    "                        pair_event += 1\n",
    "                    continue # continue to the top of for loop to skip the offshore earthquake\n",
    "                else:\n",
    "                    print('An offshore event has a score: index',i,percent_change_column[i])\n",
    "                    raise IndexError # Throws an error so one can explore this unexpected outcome\n",
    "            if np.isnan(main_events[i,-1]) == True: # Skip events with no moment tensor information\n",
    "                if extra_info[i,1] == 'yes':  # If the event is a foreshock-mainshock pair, add it to the pair matrix\n",
    "                    pair_matrix[new_index,pair_event] = 'off'\n",
    "                    pair_event += 1\n",
    "                continue # continue to the top of for loop to skip the no-moment tensor earthquake\n",
    "            if cat_end_time < main_event_times[i] + np.timedelta64(int(time_function(main_events[i,3])),'D'):\n",
    "                # Check if time of catalog download is earlier than the GK end time, if not continue\n",
    "                last_r = float(csv_data.iloc[i,-10]) # Obtain the final end time that has a score\n",
    "                if last_r < int(time_function(main_events[i,3])): # If the GK window has passed, proceed\n",
    "                    pass\n",
    "                else:\n",
    "                    # If not, the event is considered too recent to judge\n",
    "                    early += 1\n",
    "                    key = 'E'\n",
    "                    if extra_info[i,1] == 'yes':\n",
    "                        pair_matrix[new_index,pair_event] = key\n",
    "                        pair_event += 1\n",
    "                    continue\n",
    "            Event_of_interest = extra_info[i,0]\n",
    "            second_quake_exsists = extra_info[i,1]\n",
    "            if main_events[i,3] < 6.0: # Set alert-boundaries via the alert threshold\n",
    "                yg = np.copy(threshold) \n",
    "                yr = -np.copy(threshold)\n",
    "            else:\n",
    "                yg = 10\n",
    "                yr = -10\n",
    "            percent_change = percent_change_column[i]\n",
    "            if percent_change == 'None': # Check if a score was generated\n",
    "                last_r = float(csv_data.iloc[i,-10]) # Obtain the final end time that has a score\n",
    "                if last_r < r_current: # Check if the final meaningful end time occurred before the current end time\n",
    "                    prediction_list.append(default_prob)\n",
    "                    if Event_of_interest == 'first' and second_quake_exsists == 'yes':\n",
    "                        post_second_event_deadline += 1\n",
    "                        observation_list.append(0)#1=mainshock 0=foreshock\n",
    "                        key = 'I-2' # Insufficient Data - this time step is after a second event already occurred, the threat has passed\n",
    "                    else:\n",
    "                        post_gk_deadline += 1\n",
    "                        observation_list.append(1)#1=mainshock 0=foreshock\n",
    "                        key = 'I-GK' # Insufficient Data - current time is after the GK window, the threat has passed\n",
    "                else:\n",
    "                    insuff += 1\n",
    "                    key = 'I' # Insufficient Data - Code could not generate a score\n",
    "                    prediction_list.append(default_prob)\n",
    "                    if second_quake_exsists == 'yes' and Event_of_interest == 'first':\n",
    "                        observation_list.append(0)#1=mainshock 0=foreshock\n",
    "                    else:\n",
    "                        observation_list.append(1)#1=mainshock 0=foreshock\n",
    "            else: #For events that do have scores, the following determines the confusion matrix\n",
    "                percent_change = float(percent_change)\n",
    "                if Event_of_interest == 'first':\n",
    "                    if percent_change >= yg: #Green Alert\n",
    "                        if second_quake_exsists == 'yes' and Event_of_interest == 'first':\n",
    "                            failed_to_predict += 1\n",
    "                            key = 'FTP'\n",
    "                            prediction_list.append(1) #1=mainshock 0=foreshock\n",
    "                            observation_list.append(0)#1=mainshock 0=foreshock\n",
    "                        else:\n",
    "                            true_nonpredict += 1\n",
    "                            key = 'TNP'\n",
    "                            prediction_list.append(1) #1=mainshock 0=foreshock\n",
    "                            observation_list.append(1)#1=mainshock 0=foreshock\n",
    "                    elif percent_change <= yr: # Red Alert\n",
    "                        if second_quake_exsists == 'yes' and Event_of_interest == 'first':\n",
    "                            true_predict += 1\n",
    "                            key = 'TP'\n",
    "                            prediction_list.append(0) #1=mainshock 0=foreshock\n",
    "                            observation_list.append(0)#1=mainshock 0=foreshock\n",
    "                        else:\n",
    "                            false_predict += 1\n",
    "                            key = 'FP'\n",
    "                            prediction_list.append(0) #1=mainshock 0=foreshock\n",
    "                            observation_list.append(1)#1=mainshock 0=foreshock\n",
    "                    else: # Yellow Alert\n",
    "                        neutral += 1\n",
    "                        key = 'N'\n",
    "                        prediction_list.append(default_prob) # probability that this is a foreshock\n",
    "                        if second_quake_exsists == 'yes' and Event_of_interest == 'first':\n",
    "                            observation_list.append(0)\n",
    "                        else:\n",
    "                            observation_list.append(1)\n",
    "                else:\n",
    "                    if percent_change >= yg: # Green Alert\n",
    "                        prediction_list.append(1)#1=mainshock 0=foreshock\n",
    "                        if second_quake_exsists == 'yes' and Event_of_interest == 'first':\n",
    "                            failed_to_predict += 1\n",
    "                            key = 'FTP'\n",
    "                            observation_list.append(0)#1=mainshock 0=foreshock\n",
    "                        else:\n",
    "                            true_nonpredict += 1\n",
    "                            key = 'TNP'\n",
    "                            observation_list.append(1)#1=mainshock 0=foreshock\n",
    "                    elif percent_change <= yr: # Red Alert\n",
    "                        prediction_list.append(0)#1=mainshock 0=foreshock\n",
    "                        if second_quake_exsists == 'yes' and Event_of_interest == 'first':\n",
    "                            true_predict += 1\n",
    "                            key = 'TP'\n",
    "                            observation_list.append(0)#1=mainshock 0=foreshock\n",
    "                        else:\n",
    "                            false_predict += 1\n",
    "                            key = 'FP'\n",
    "                            observation_list.append(1)#1=mainshock 0=foreshock\n",
    "                    else: # Yellow Alert \n",
    "                        neutral += 1\n",
    "                        key = 'N'\n",
    "                        prediction_list.append(default_prob)\n",
    "                        if second_quake_exsists == 'yes' and Event_of_interest == 'first':\n",
    "                            observation_list.append(0)\n",
    "                        else:\n",
    "                            observation_list.append(1)\n",
    "            if extra_info[i,1] == 'yes':  # If the event is a foreshock-mainshock pair, add it to the pair matrix\n",
    "                pair_matrix[new_index,pair_event] = key\n",
    "                pair_event += 1\n",
    "        prediction_list = np.array(prediction_list)\n",
    "        observation_list = np.array(observation_list)\n",
    "        candidate_tp = len(observation_list) - np.count_nonzero(observation_list) # True number of foreshocks\n",
    "        C = calculate_MatthewsC(prediction_list,observation_list)\n",
    "        ktb = stats.kendalltau(x=prediction_list,y=observation_list)[0]\n",
    "        print('\\nThreshold = ',threshold)                 \n",
    "        print('Number of Successful Predictions:',true_predict)\n",
    "        print('Number of Successful Non-Predictions:',true_nonpredict)\n",
    "        print('Number of Incorrect Predictions:',false_predict)\n",
    "        print('Number of Fail to Predicts:',failed_to_predict)\n",
    "        print('Number of Neutral Results:',neutral)\n",
    "        print('Too Early to Call:',early)\n",
    "        print('Total Events post-GK Cutoff:',post_gk_deadline)\n",
    "        print('Total Events post-2nd Event Cutoff:',post_second_event_deadline)\n",
    "        print('Total Events with Insufficient Data:',insuff)\n",
    "        print('Total:',true_predict+true_nonpredict+false_predict+failed_to_predict+neutral+early+post_gk_deadline+post_second_event_deadline+insuff)\n",
    "        conf_list.append([true_predict,true_nonpredict,false_predict,failed_to_predict,neutral,early,post_gk_deadline+post_second_event_deadline+insuff])\n",
    "                \n",
    "        # Calculate F-score with beta = 2\n",
    "        beta = 2\n",
    "        if true_predict == 0 and false_predict == 0 and failed_to_predict == 0:\n",
    "            f_2 = 0.0\n",
    "        else:\n",
    "            \n",
    "            f_2 = (((1+beta**2)*true_predict) / (((1+beta**2)*true_predict) + (((((beta**2)*((len(observation_list)-candidate_tp)-true_nonpredict))+(candidate_tp-true_predict))))))\n",
    "        f2_list.append(f_2)\n",
    "        # Calculate F-score with beta = 1\n",
    "        beta = 1\n",
    "        if true_predict == 0 and false_predict == 0 and failed_to_predict == 0:\n",
    "            f_1 = 0.0\n",
    "        else:\n",
    "            f_1 = (((1+beta**2)*true_predict) / (((1+beta**2)*true_predict) + (((((beta**2)*((len(observation_list)-candidate_tp)-true_nonpredict))+(candidate_tp-true_predict))))))\n",
    "        f1_list.append(f_1)\n",
    "        # Calculate Balanced Accuracy\n",
    "        if (true_predict + failed_to_predict) == 0 or (true_nonpredict + false_predict) == 0:\n",
    "            ba = 0.0\n",
    "        else:\n",
    "            tp_rate = true_predict / candidate_tp\n",
    "            tn_rate = true_nonpredict / (len(observation_list)-candidate_tp)\n",
    "            ba = (tp_rate+tn_rate) / 2\n",
    "        ba_list.append(ba)\n",
    "        C_list.append(C)\n",
    "        ktb_list.append(ktb)\n",
    "        new_index += 1\n",
    "    return conf_list, pair_matrix, f2_list, f1_list, ba_list, C_list, ktb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Matrix\n",
    "\n",
    "Col 0: MCC\n",
    "\n",
    "Col 1: start time\n",
    "\n",
    "Col 2: no alert time\n",
    "\n",
    "Col 3: precut magnitude\n",
    "\n",
    "Col 4: distance threshold\n",
    "\n",
    "Col 5: alert threshold\n",
    "\n",
    "Col 6: end time\n",
    "\n",
    "Col 7: F1\n",
    "\n",
    "Col 8: F2\n",
    "\n",
    "Col 9: Balanced Average\n",
    "\n",
    "Col 10: Kendall Tau Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix:\n",
    "\n",
    "Col 0: True Predictions (TP)\n",
    "\n",
    "Col 1: True Non-predictions (TNP)\n",
    "\n",
    "Col 2: False Predict (FP)\n",
    "\n",
    "Col 3: Fail to Predict (FTP)\n",
    "\n",
    "Col 4: Neutral (N)\n",
    "\n",
    "Col 5: Early (E)\n",
    "\n",
    "Col 6: Insufficient (Ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_list_mod = [0.25,0.5,0.75,1,2,3,4,7,15,30,45,60,90,120,150,180,365,730]\n",
    "n = 720 * len(r_list_mod) * 5 # Number of unique parameter combinations\n",
    "result_matrix = np.empty((n,11)) # Initialize matrix to hold parameter combinations and statistial scores, \n",
    "conf_matrix = np.empty((n,7)) # Initialize matrix to hold the confusion matrix results \n",
    "pair_matrix = np.empty((n,np.count_nonzero(extra_info[:,1] == 'yes')),dtype=object) # Initialize matrix to hold the confusion matrix result for the mainshock-foreshock pairs\n",
    "index = 0\n",
    "for filename in os.listdir():\n",
    "    if filename.startswith('st'):\n",
    "        csv_data = pd.read_csv(filename,dtype=str)\n",
    "        fields = filename[:-4].split('_') # Extract the parameter combination from the file name\n",
    "        fields = [j.replace('Auto','-1') for j in fields]\n",
    "        for r in r_list_mod: # Loop through each time step\n",
    "            print('Time Step:',r)\n",
    "            conf_values, pair_matrix, f2_list, f1_list, ba_list, C_list, ktb_list = calculate_accuracy(grand_times,csv_data,r,extra_info,main_event_times,main_events,r_list,pair_matrix,index,offshore_indicies)\n",
    "            for count,i in enumerate(C_list):\n",
    "                # Populate result_matrix with the statistial scores and parameter combinations\n",
    "                result_matrix[index,0] = i\n",
    "                result_matrix[index,1] = fields[1]\n",
    "                result_matrix[index,2] = fields[3]\n",
    "                result_matrix[index,3] = fields[5]\n",
    "                result_matrix[index,4] = fields[7]\n",
    "                result_matrix[index,6] = r\n",
    "                result_matrix[index,7] = f2_list[count]\n",
    "                result_matrix[index,8] = f1_list[count]\n",
    "                result_matrix[index,9] = ba_list[count]\n",
    "                result_matrix[index,10] = ktb_list[count]\n",
    "                # Populate the conf_matrix with the confusion matrix values\n",
    "                conf_matrix[index,0] = conf_values[count][0]\n",
    "                conf_matrix[index,1] = conf_values[count][1]\n",
    "                conf_matrix[index,2] = conf_values[count][2]\n",
    "                conf_matrix[index,3] = conf_values[count][3]\n",
    "                conf_matrix[index,4] = conf_values[count][4]\n",
    "                conf_matrix[index,5] = conf_values[count][5]\n",
    "                conf_matrix[index,6] = conf_values[count][6]\n",
    "                # Adding the alert threshold to the result_matrix\n",
    "                if count == 0:\n",
    "                    result_matrix[index,5] = 5\n",
    "                elif count == 1:\n",
    "                    result_matrix[index,5] = 10\n",
    "                elif count == 2:\n",
    "                    result_matrix[index,5] = 15\n",
    "                elif count == 3:\n",
    "                    result_matrix[index,5] = 20\n",
    "                else:\n",
    "                    result_matrix[index,5] = 25\n",
    "                index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_master_list = np.copy(result_matrix[:,0])\n",
    "plt.hist(c_master_list,bins=10)\n",
    "plt.xlabel('MCC')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sort by MCC, use index = 0 below\n",
    "\n",
    "f2 use index = 7\n",
    "\n",
    "f1, index = 8\n",
    "\n",
    "balanced average, index = 9\n",
    "\n",
    "Kendall's Tau, index=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "for i in range(len(result_matrix[:,10])):\n",
    "    if np.isnan(result_matrix[i,10]) == True:\n",
    "        result_matrix[i,10] = -2 # Removing nans because argsort() is ranking nans the highest\n",
    "sorted_array = result_matrix[result_matrix[:,index].argsort()]\n",
    "sorted_matrix = conf_matrix[result_matrix[:,index].argsort()]\n",
    "sorted_pairs = pair_matrix[result_matrix[:,index].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "headers = ['C','Start Time','No Alert Time (days)','Precut Magnitude','Distance Threshold (km)','Alert Threshold','End Time (days)','TP','TNP','FP','FTP','N','E','Ins','F2','F1','BA','KTB']\n",
    "table = tabulate(np.concatenate([sorted_array[-13:,[0,1,2,3,4,5,6]],sorted_matrix[-13:,:],sorted_array[-13:,[7,8,9,10]]],axis=-1),headers=headers,tablefmt=\"latex\",floatfmt=(\".3f\",'.0f','.1f','.1f','.1f','.0f','.2f','.0f','.0f','.0f','.0f','.0f','.0f','.0f',\".3f\",\".3f\",\".3f\",\".3f\")) \n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['C','Start Time','No Alert Time (days)','Precut Magnitude','Distance Threshold (km)','Alert Threshold','End T']\n",
    "table = tabulate(sorted_array[-13:,[0,1,2,3,4,5,6]],headers=headers,tablefmt=\"simple\") #-21\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers1 = ['TP','TNP','FP','FTP','N','E','Ins']\n",
    "table1 = tabulate(sorted_matrix[-13:,:],headers=headers1,tablefmt='simple')\n",
    "print(table1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of select candidate event pairs\n",
    "\n",
    "I-GK = Gardner Knopoff Window ended before the end-time (threat has passed), I-2 = The initial event was a foreshock and the mainshock occurred before the current end time, so this alert is meaningless. Both I-GK and I-2 are grouped with insufficient data for the score calculation. I = insufficient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers2 = ['5.9','Chalf.','JT','Lan','5.36','5.74','Rid','Rid2']\n",
    "table2 = tabulate(sorted_pairs[-13:,[2,3,14,15,20,21,37,38]],headers=headers2,tablefmt='simple')\n",
    "print(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find Indicies of Earthquakes that ever generate a True Prediction\n",
    "col_list = []\n",
    "for row in range(len(sorted_pairs[:,0])):\n",
    "    for col in range(len(sorted_pairs[0,:])):\n",
    "        if sorted_pairs[row,col] == 'TP':\n",
    "            if col not in col_list:\n",
    "                col_list.append(col)\n",
    "print(col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Time Variability\n",
    "\n",
    "This cycles through every parameter combination, and considers each individual earthquake to determine how the alert level changes with end time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counts of all possibilities\n",
    "g_r = 0 # Changes between green and red, vice versa\n",
    "r_y = 0 # Between red and yellow, vice versa\n",
    "g_y = 0 # Between green and yellow, vice versa\n",
    "g_all = 0 # Always green\n",
    "y_all = 0 # Always red\n",
    "r_all = 0 # Always red\n",
    "i_all = 0 # Always insufficient\n",
    "csv_count = 0\n",
    "M6_count = 0\n",
    "for filename in os.listdir():\n",
    "    if filename.startswith('st'):\n",
    "        csv_count += 1\n",
    "        csv_data = pd.read_csv(filename,dtype=str)\n",
    "        i_list = []\n",
    "        for i in range(len(csv_data.iloc[:,0])):\n",
    "            if i in offshore_indicies: # skip offshore earthquakes\n",
    "                continue\n",
    "            if np.isnan(main_events[i,-1]) == True: #skip quakes with no moment tensor\n",
    "                continue\n",
    "            row = csv_data.iloc[i,:]\n",
    "            M6_count += 1\n",
    "            # Initialize a set of keys to mark colors at previous time steps\n",
    "            red_permanent_key = False # Permanent keys are turned on if the color ever appears, and do not get overwritten\n",
    "            green_permanent_key = False\n",
    "            yellow_permanent_key = False\n",
    "            red_key = False # These keys are overwritten at every end time\n",
    "            green_key = False\n",
    "            yellow_key = False\n",
    "            for time_index in range(len(r_list_mod)):\n",
    "                percent_change = row[8+(9*time_index)]\n",
    "                if percent_change != 'None':\n",
    "                    percent_change = float(percent_change)\n",
    "                    if percent_change <= -10:\n",
    "                        red_key = True\n",
    "                    elif percent_change >= 10:\n",
    "                        green_key = True\n",
    "                    else:\n",
    "                        yellow_key = True\n",
    "                    if red_permanent_key == False:\n",
    "                        if red_key == True:\n",
    "                            red_permanent_key = True\n",
    "                    if green_permanent_key == False:\n",
    "                        if green_key == True:\n",
    "                            green_permanent_key = True\n",
    "                    if yellow_permanent_key == False:\n",
    "                        if yellow_key == True:\n",
    "                            yellow_permanent_key = True\n",
    "            if (green_permanent_key == True) and (yellow_permanent_key == False) and (red_permanent_key == False):\n",
    "                g_all += 1\n",
    "            elif (green_permanent_key == False) and (yellow_permanent_key == True) and (red_permanent_key == False):\n",
    "                y_all += 1\n",
    "            elif (green_permanent_key == False) and (yellow_permanent_key == False) and (red_permanent_key == True):\n",
    "                r_all += 1\n",
    "            elif (green_permanent_key == True) and (yellow_permanent_key == True) and (red_permanent_key == False):\n",
    "                g_y += 1\n",
    "            elif (green_permanent_key == False) and (yellow_permanent_key == True) and (red_permanent_key == True):\n",
    "                r_y += 1\n",
    "            elif np.all(i_list == 'i'):\n",
    "                i_all += 1\n",
    "            elif (green_permanent_key == True) and (red_permanent_key == True):\n",
    "                g_r += 1\n",
    "            elif (green_permanent_key == False) and (yellow_permanent_key == False) and (red_permanent_key == False):\n",
    "                i_all += 1\n",
    "            else:\n",
    "                print(green_permanent_key,red_permanent_key,yellow_permanent_key)\n",
    "                    \n",
    "                \n",
    "total = g_all + r_all + y_all + i_all + g_y + r_y + g_r                \n",
    "print('Number that are Always Green:', g_all, '|',100*(g_all/total),'%')\n",
    "print('Number that are Always Red:', r_all, '|',100*(r_all/total),'%')\n",
    "print('Number that are Always Yellow:', y_all, '|',100*(y_all/total),'%')\n",
    "print('Number that Change between Green and Yellow:',g_y, '|',100*(g_y/total),'%')\n",
    "print('Number that Change between Red and Yellow:',r_y, '|',100*(r_y/total),'%')\n",
    "print('Number that Change between Green and Red:',g_r, '|',100*(g_r/total),'%')\n",
    "print('Number that Are Insuff at each time step:',i_all, '|',100*(i_all/total),'%')\n",
    "print('Csv Files Checked:',csv_count)\n",
    "print(\"Percent Consistent:\",100*((g_all+r_all+y_all)/total))\n",
    "print(\"Percent that Change:\",100*((g_y+r_y+g_r)/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best runs, and construct their appropriate file names\n",
    "best_runs = np.copy(sorted_array[-1:,:])\n",
    "best_run_files = []\n",
    "for i in range(len(best_runs)):\n",
    "    st = best_runs[i,1]\n",
    "    na = best_runs[i,2]\n",
    "    pm = best_runs[i,3]\n",
    "    dt = best_runs[i,4]\n",
    "    if st == -1:\n",
    "        st = 'Auto'\n",
    "    if na == -1:\n",
    "        na = 'Auto'\n",
    "    if pm == -1:\n",
    "        pm = 'Auto'\n",
    "    if st != 1971 and st != 'Auto':\n",
    "        st = '{:.1f}'.format(st)\n",
    "    elif st == 1971:\n",
    "        st = '1971'\n",
    "    if na != 0.05 and st != 'Auto':\n",
    "        na = '{:.1f}'.format(na)\n",
    "    elif na == 0.05:\n",
    "        na = str(na)\n",
    "    if pm != 'Auto':\n",
    "        pm = '{:.1f}'.format(pm)\n",
    "    file = 'st_'+st+'_na_'+str(na)+'_pm_'+pm+'_dt_'+str(dt)+'.csv'\n",
    "    if file not in best_run_files:\n",
    "        best_run_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the time-variability analysis, but for only the best runs\n",
    "g_r = 0\n",
    "r_y = 0\n",
    "g_y = 0\n",
    "g_all = 0\n",
    "y_all = 0\n",
    "r_all = 0\n",
    "csv_count = 0\n",
    "i_all = 0\n",
    "M6_count = 0\n",
    "for filename in best_run_files:             \n",
    "    csv_count += 1\n",
    "    csv_data = pd.read_csv(filename,dtype=str)\n",
    "    i_list = []\n",
    "    for i in range(len(csv_data.iloc[:,0])):\n",
    "        if i in offshore_indicies: # skip offshore earthquakes\n",
    "                continue\n",
    "        if np.isnan(main_events[i,-1]) == True: # Skip quakes that don't have a focal mechanism\n",
    "                continue\n",
    "        row = csv_data.iloc[i,:]\n",
    "        M6_count += 1\n",
    "        red_permanent_key = False\n",
    "        green_permanent_key = False\n",
    "        yellow_permanent_key = False\n",
    "        red_key = False\n",
    "        green_key = False\n",
    "        yellow_key = False\n",
    "        for time_index in range(len(r_list_mod)):\n",
    "            percent_change = row[8+(9*time_index)]\n",
    "            if percent_change != 'None':\n",
    "                percent_change = float(percent_change)\n",
    "                if percent_change <= -10:\n",
    "                    red_key = True\n",
    "                elif percent_change >= 10:\n",
    "                    green_key = True\n",
    "                else:\n",
    "                    yellow_key = True\n",
    "                if red_permanent_key == False:\n",
    "                    if red_key == True:\n",
    "                        red_permanent_key = True\n",
    "                if green_permanent_key == False:\n",
    "                    if green_key == True:\n",
    "                        green_permanent_key = True\n",
    "                if yellow_permanent_key == False:\n",
    "                    if yellow_key == True:\n",
    "                        yellow_permanent_key = True\n",
    "        if (green_permanent_key == True) and (yellow_permanent_key == False) and (red_permanent_key == False):\n",
    "            g_all += 1\n",
    "        elif (green_permanent_key == False) and (yellow_permanent_key == True) and (red_permanent_key == False):\n",
    "            y_all += 1\n",
    "        elif (green_permanent_key == False) and (yellow_permanent_key == False) and (red_permanent_key == True):\n",
    "            r_all += 1\n",
    "        elif (green_permanent_key == True) and (yellow_permanent_key == True) and (red_permanent_key == False):\n",
    "            g_y += 1\n",
    "        elif (green_permanent_key == False) and (yellow_permanent_key == True) and (red_permanent_key == True):\n",
    "            r_y += 1\n",
    "        elif np.all(i_list == 'i'):\n",
    "            i_all += 1\n",
    "        elif (green_permanent_key == True) and (red_permanent_key == True):\n",
    "            g_r += 1\n",
    "        elif (green_permanent_key == False) and (yellow_permanent_key == False) and (red_permanent_key == False):\n",
    "            i_all += 1\n",
    "        else:\n",
    "            print(green_permanent_key,red_permanent_key,yellow_permanent_key)\n",
    "\n",
    "\n",
    "total = g_all + r_all + y_all + i_all + g_y + r_y + g_r                \n",
    "print('Number that are Always Green:', g_all, '|',100*(g_all/total),'%')\n",
    "print('Number that are Always Red:', r_all, '|',100*(r_all/total),'%')\n",
    "print('Number that are Always Yellow:', y_all, '|',100*(y_all/total),'%')\n",
    "print('Number that Change between Green and Yellow:',g_y, '|',100*(g_y/total),'%')\n",
    "print('Number that Change between Red and Yellow:',r_y, '|',100*(r_y/total),'%')\n",
    "print('Number that Change between Green and Red:',g_r, '|',100*(g_r/total),'%')\n",
    "print('Number that Are Insuff at each time step:',i_all, '|',100*(i_all/total),'%')\n",
    "print('Csv Files Checked:',csv_count)\n",
    "print(\"Percent Consistent:\",100*((g_all+r_all+y_all)/total))\n",
    "print(\"Percent that Change:\",100*((g_y+r_y+g_r)/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty\n",
    "\n",
    "This cycles through every parameter combination, and considers each alert to determine if the uncertainty spans multiple alert levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counts of all possibilities\n",
    "g_r = 0 # Uncertainty spans both green and red\n",
    "r_y = 0 # Uncertainty spans red and yellow\n",
    "g_y = 0 # Uncertainty spans green and yellow\n",
    "g_all = 0 # Uncertainty is fully contained within green\n",
    "y_all = 0 # Uncertainty is fully contained within yellow\n",
    "r_all = 0 # Uncertainty is fully contained within red\n",
    "csv_count = 0\n",
    "i_all = 0 # Insufficient\n",
    "M6_count = 0\n",
    "for filename in os.listdir():\n",
    "    if filename.startswith('st'):\n",
    "        csv_count += 1\n",
    "        csv_data = pd.read_csv(filename,dtype=str)\n",
    "        i_list = []\n",
    "        for i in range(len(csv_data.iloc[:,0])):\n",
    "            if i in offshore_indicies: # skip offshore earthquakes\n",
    "                continue\n",
    "            if np.isnan(main_events[i,-1]) == True: # Skip quakes that don't have a focal mechanism\n",
    "                continue\n",
    "            row = csv_data.iloc[i,:]\n",
    "            M6_count += 1\n",
    "            red_key = False # Set of keys to determine what colors the uncertainty bars extend into\n",
    "            green_key = False\n",
    "            yellow_key = False\n",
    "            for time_index in range(len(r_list_mod)):\n",
    "                percent_change = row[8+(9*time_index)]\n",
    "                if percent_change != 'None':\n",
    "                    percent_change = float(percent_change)\n",
    "                    pc_unc = float(row[9+(9*time_index)])\n",
    "                    if percent_change <= -10: # Get the alert level of the original alert\n",
    "                        red_key = True\n",
    "                    elif percent_change >= 10:\n",
    "                        green_key = True\n",
    "                    else:\n",
    "                        yellow_key = True\n",
    "                    upp_bound = percent_change + pc_unc\n",
    "                    if upp_bound <= -10: # Get the alert level of the upper bound uncertainty\n",
    "                        red_key = True\n",
    "                    elif upp_bound >= 10:\n",
    "                        green_key = True\n",
    "                    else:\n",
    "                        yellow_key = True\n",
    "                    low_bound = percent_change - pc_unc\n",
    "                    if low_bound <= -10: # Get the alert level of the lower bound uncertainty\n",
    "                        red_key = True\n",
    "                    elif low_bound >= 10:\n",
    "                        green_key = True\n",
    "                    else:\n",
    "                        yellow_key = True\n",
    "                    if (green_key == True) and (yellow_key == False) and (red_key == False):\n",
    "                        g_all += 1\n",
    "                    elif (green_key == False) and (yellow_key == True) and (red_key == False):\n",
    "                        y_all += 1\n",
    "                    elif (green_key == False) and (yellow_key == False) and (red_key == True):\n",
    "                        r_all += 1\n",
    "                    elif (green_key == True) and (yellow_key == True) and (red_key == False):\n",
    "                        g_y += 1\n",
    "                    elif (green_key == False) and (yellow_key == True) and (red_key == True):\n",
    "                        r_y += 1\n",
    "                    elif np.all(i_list == 'i'):\n",
    "                        i_all += 1\n",
    "                    elif (green_key == True) and (red_key == True):\n",
    "                        g_r += 1\n",
    "                    else:\n",
    "                        print(green_permanent_key,red_permanent_key,yellow_permanent_key)\n",
    "\n",
    "                else:\n",
    "                    i_all += 1\n",
    "\n",
    "                \n",
    "total = g_all + r_all + y_all + i_all + g_y + r_y + g_r                \n",
    "print('Number that are Always Green:', g_all, '|',100*(g_all/total),'%')\n",
    "print('Number that are Always Red:', r_all, '|',100*(r_all/total),'%')\n",
    "print('Number that are Always Yellow:', y_all, '|',100*(y_all/total),'%')\n",
    "print('Number that Change between Green and Yellow:',g_y, '|',100*(g_y/total),'%')\n",
    "print('Number that Change between Red and Yellow:',r_y, '|',100*(r_y/total),'%')\n",
    "print('Number that Change between Green and Red:',g_r, '|',100*(g_r/total),'%')\n",
    "print('Number that Are Insuff:',i_all, '|',100*(i_all/total),'%')\n",
    "print('Csv Files Checked:',csv_count)\n",
    "print(\"Percent Consistent:\",100*((g_all+r_all+y_all)/total))\n",
    "print(\"Percent that Change:\",100*((g_y+r_y+g_r)/total))\n",
    "print('Total:',total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the uncertainty analysis for the best runs\n",
    "g_r = 0\n",
    "r_y = 0\n",
    "g_y = 0\n",
    "g_all = 0\n",
    "y_all = 0\n",
    "r_all = 0\n",
    "csv_count = 0\n",
    "i_all = 0\n",
    "M6_count = 0\n",
    "for filename in best_run_files:\n",
    "    csv_count += 1\n",
    "    csv_data = pd.read_csv(filename,dtype=str)\n",
    "    i_list = []\n",
    "    for i in range(len(csv_data.iloc[:,0])):\n",
    "        if i in offshore_indicies: # skip offshore earthquakes\n",
    "                continue\n",
    "        if np.isnan(main_events[i,-1]) == True: # Skip quakes that don't have a focal mechanism\n",
    "                continue\n",
    "        row = csv_data.iloc[i,:]\n",
    "        M6_count += 1\n",
    "        red_key = False\n",
    "        green_key = False\n",
    "        yellow_key = False\n",
    "        for time_index in range(len(r_list_mod)):\n",
    "            percent_change = row[8+(9*time_index)]\n",
    "            if percent_change != 'None':\n",
    "                percent_change = float(percent_change)\n",
    "                pc_unc = float(row[9+(9*time_index)])\n",
    "                if percent_change <= -10:\n",
    "                    red_key = True\n",
    "                elif percent_change >= 10:\n",
    "                    green_key = True\n",
    "                else:\n",
    "                    yellow_key = True\n",
    "                upp_bound = percent_change + pc_unc\n",
    "                if upp_bound <= -10:\n",
    "                    red_key = True\n",
    "                elif upp_bound >= 10:\n",
    "                    green_key = True\n",
    "                else:\n",
    "                    yellow_key = True\n",
    "                low_bound = percent_change - pc_unc\n",
    "                if low_bound <= -10:\n",
    "                    red_key = True\n",
    "                elif low_bound >= 10:\n",
    "                    green_key = True\n",
    "                else:\n",
    "                    yellow_key = True\n",
    "                if (green_key == True) and (yellow_key == False) and (red_key == False):\n",
    "                    g_all += 1\n",
    "                elif (green_key == False) and (yellow_key == True) and (red_key == False):\n",
    "                    y_all += 1\n",
    "                elif (green_key == False) and (yellow_key == False) and (red_key == True):\n",
    "                    r_all += 1\n",
    "                elif (green_key == True) and (yellow_key == True) and (red_key == False):\n",
    "                    g_y += 1\n",
    "                elif (green_key == False) and (yellow_key == True) and (red_key == True):\n",
    "                    r_y += 1\n",
    "                elif np.all(i_list == 'i'):\n",
    "                    i_all += 1\n",
    "                elif (green_key == True) and (red_key == True):\n",
    "                    g_r += 1\n",
    "                else:\n",
    "                    print(green_permanent_key,red_permanent_key,yellow_permanent_key)\n",
    "\n",
    "            else:\n",
    "                i_all += 1\n",
    "\n",
    "                \n",
    "total = g_all + r_all + y_all + i_all + g_y + r_y + g_r                \n",
    "print('Number that are Always Green:', g_all, '|',100*(g_all/total),'%')\n",
    "print('Number that are Always Red:', r_all, '|',100*(r_all/total),'%')\n",
    "print('Number that are Always Yellow:', y_all, '|',100*(y_all/total),'%')\n",
    "print('Number that Change between Green and Yellow:',g_y, '|',100*(g_y/total),'%')\n",
    "print('Number that Change between Red and Yellow:',r_y, '|',100*(r_y/total),'%')\n",
    "print('Number that Change between Green and Red:',g_r, '|',100*(g_r/total),'%')\n",
    "print('Number that Are Insuff:',i_all, '|',100*(i_all/total),'%')\n",
    "print('Csv Files Checked:',csv_count)\n",
    "print(\"Percent Consistent:\",100*((g_all+r_all+y_all)/total))\n",
    "print(\"Percent that Change:\",100*((g_y+r_y+g_r)/total))\n",
    "print('Total:',total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
